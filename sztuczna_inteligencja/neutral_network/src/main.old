// use plotters::prelude::*;
use rand::Rng;

fn sigmoid(x: f64) -> f64{
    1f64/(1f64+f64::exp(-x)) // 1/(1-e^-x)
}

fn sigmoid_deriative(x: f64) -> f64{
    f64::exp(x)/((1f64+f64::exp(x)).powi(2))
}

#[allow(dead_code)]
fn calc_node(params: &Vec<f64>, weights: &Vec<f64>) -> f64 {
    params.iter().zip(weights.iter()).map(|(x, y)| x * y).sum()  // techinicznie może da się to zrobić lepiej ale ja na ten moment tego nie postrafie
}


fn learn(inputs: &Vec<Vec<f64>>, target: &Vec<f64>,
        weights: &mut Vec<Vec<f64>>, bias: &mut Vec<f64>,
        out_weights: &mut Vec<f64>, activator: fn(f64) -> f64,
        activator_deriative: fn(f64) -> f64, momentum: bool,
        epochs: usize, n: f64, alpha: f64) {
    // main function for learing 
    
    let mut vm: Vec<Vec<f64>>= weights.iter().map(|x|{ x.iter().map(|_y| {1f64}).collect()}).collect();
    
    let mut wm: Vec<f64> = out_weights.iter().map(|_x|{1f64}).collect();

    let mut history: Vec<f64> = vec![];
    let mut iter = 0;
    while iter<epochs{
        let mut error: f64 = 0.0;
        for (inp, out) in inputs.iter().zip(target){
            let h1: f64 = calc_node(inp, &weights[0]) - bias[0];
            let y1: f64 = activator(h1);
            let h2: f64 = calc_node(inp, &weights[1]) + bias[1];
            let y2: f64 = activator(h2);
            let pred: f64 = activator(h1) * out_weights[0] + activator(h2) * out_weights[1] + bias[2]; // before
            let pred_out: f64 = activator(pred); // network output after activation
            // activation

            error = error + 0.5*(out - pred_out).powi(2); //
            
            let deltaz: f64 = (out-pred_out)*activator_deriative(pred_out);
            let sumdw: f64 = deltaz*out_weights[0]+deltaz*out_weights[1];
            let deltay1: f64 = activator_deriative(h1)*sumdw;
            let deltay2: f64 = activator_deriative(h2)*sumdw;
            
            if momentum == true {

                // Output layer
                out_weights[0] += n*deltaz*y1+alpha*wm[0];
                wm[0] = n*deltaz*y1;
                out_weights[1] += n*deltaz*y2+alpha*wm[1];
                wm[1] = n*deltaz*y2;

                // hidden layers
                weights[0][0] += n*deltay1*inp[0] + alpha*vm[0][0]; // w11 == w00 
                vm[0][0] = n*deltay1*inp[0]; 
                weights[0][1] += n*deltay2*inp[1] + alpha*vm[0][1]; // w12 == w01
                vm[0][0] = n*deltay1*inp[1]; 
                weights[1][0] += n*deltay1*inp[0] + alpha*vm[1][0]; // w21 == w10
                vm[0][0] = n*deltay1*inp[0]; 
                weights[1][1] += n*deltay2*inp[1] + alpha*vm[1][1]; // w22 == w11
                vm[0][0] = n*deltay1*inp[1];
            }
            else {

                out_weights[0] += n*deltaz*h1;
                out_weights[1] += n*deltaz*h2;

                weights[0][0] += n*deltay1*inp[0]; // w11 == w00 
                weights[0][1] += n*deltay2*inp[1]; // w12 == w01
                weights[1][0] += n*deltay1*inp[0]; // w21 == w10
                weights[1][1] += n*deltay2*inp[1]; // w22 == w11
                
            }

            }
        iter += 1;
        history.push(error);
        println!("Iter {}: error: {}",iter,error);
    }
}

#[allow(dead_code)]
fn main() {
    let mut rng = rand::thread_rng();

    // declaring constans to adjust later
    const VARIANCE_W: f64 = 0.5f64;
    const EPOCHS: usize = 100000;
    
    // declaring random weights for inputs nodes
    // [[w11,w12], [w21,w22], [b1,b2]]
    let mut weights: Vec<Vec<f64>> = vec![
                                        vec![rng.gen::<f64>(),rng.gen::<f64>()], // w11, w12,
                                        vec![rng.gen::<f64>(),rng.gen::<f64>()], // w21, w22
    ];
    let mut bias =  vec![-1f64,-1f64,-1f64]; // b1 b2 b3

    let mut output_weights: Vec<f64> = vec![rng.gen::<f64>(),rng.gen::<f64>()]; //o1 o2
    
    let inputs: Vec<Vec<f64>> = vec![
        vec![0f64,0f64],
        vec![0f64,1f64],
        vec![1f64,0.0],
        vec![1.0,1.0]
    ];
    
    let outputs: Vec<f64> = vec![0.0,1.0,1.0,0.0];
    
     
    let val:f64 = sigmoid(1f64);
    /*
    * fn learn(inputs: &Vec<Vec<f64>>, target: &Vec<f64>,
        weights: &mut Vec<Vec<f64>>, bias: &mut Vec<f64>,
        out_weights: &mut Vec<f64>, activator: fn(f64) -> f64,
        activator_deriative: fn(f64) -> f64, momentum: bool,
        epochs: usize, n: f64, alpha: f64) {

    */

    learn(
        &inputs,
        &outputs,
        &mut weights, 
        &mut bias, 
        &mut output_weights, 
        sigmoid,
        sigmoid_deriative,
        false,
        EPOCHS,
        0.5,
        0.9
    );
    
    println!("{}", val);
}
